{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "offshore-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "PATH = '/home/arsentii/prog/camda/'\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_sci_md\", disable=['ner', 'parser'])\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rough-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(PATH + 'DILI_data.csv')\n",
    "validation_data = pd.read_csv(PATH + \"Validation.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "romance-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    doc = nlp.make_doc(string)\n",
    "    words = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop and len(token.text) > 1 ]\n",
    "    return words\n",
    "\n",
    "def tokenization(train_data):\n",
    "    tokenized_texts = []\n",
    "    for _, row in train_data.iterrows():\n",
    "        text = row['Title'] + ' ' + str(row['Abstract'])\n",
    "        words = tokenize(text)\n",
    "        tokenized_texts.append(words)\n",
    "    return tokenized_texts\n",
    "\n",
    "# TFIDF (Term frequency and inverse document frequency)\n",
    "\n",
    "def get_word_stat(tokenized_texts):\n",
    "    '''Words counts in documents\n",
    "    finds in how many documents this word \n",
    "    is present\n",
    "    '''\n",
    "    texts_number = len(tokenized_texts)\n",
    "    word2text_count = defaultdict(int)\n",
    "    for text in tokenized_texts:\n",
    "        uniquewords = set(text)\n",
    "        for word in uniquewords:\n",
    "            word2text_count[word] +=1\n",
    "    return word2text_count\n",
    "\n",
    "def get_doc_tfidf(words, word2text_count, N):\n",
    "    num_words = len(words)\n",
    "    word2tfidf = defaultdict(int)\n",
    "    for word in words:\n",
    "        if word2text_count[word] > 0:\n",
    "            idf = np.log(N/(word2text_count[word]))\n",
    "            word2tfidf[word] += (1/num_words) * idf\n",
    "        else:\n",
    "            word2tfidf[word] = 1\n",
    "    return word2tfidf\n",
    "\n",
    "def create_pmi_dict(tokenized_texts, targets, min_count=5):\n",
    "    np.seterr(divide = 'ignore')\n",
    "    \n",
    "    # words count\n",
    "    d = {0:defaultdict(int), 1:defaultdict(int), 'tot':defaultdict(int)}\n",
    "    for idx, words in enumerate(tokenized_texts):\n",
    "        target = targets[idx]\n",
    "        for w in words:\n",
    "            d[ target ][w] += 1\n",
    "            \n",
    "    Dictionary = set(list(d[0].keys()) + list(d[1].keys()))\n",
    "    d['tot'] = {w:d[0][w] + d[1][w] for w in Dictionary}\n",
    "    \n",
    "    # pmi calculation\n",
    "    N_0 = sum(d[0].values())\n",
    "    N_1 = sum(d[1].values())\n",
    "    d[0] = {w: -np.log((v/N_0 + 10**(-15)) / (0.5 * d['tot'][w]/(N_0 + N_1))) / np.log(v/N_0 + 10**(-15))\n",
    "            for w, v in d[0].items() if d['tot'][w] > min_count}\n",
    "    \n",
    "    d[1] = {w: -np.log((v/N_1+ 10**(-15)) / (0.5 * d['tot'][w]/(N_0 + N_1))) / np.log(v/N_1 + 10**(-15))\n",
    "            for w, v in d[1].items() if d['tot'][w] > min_count}\n",
    "    del d['tot']\n",
    "    return d    \n",
    "\n",
    "def classify_pmi_based(words_pmis, word2text_count, tokenized_test_texts, N):\n",
    "    results = np.zeros(len(tokenized_test_texts))\n",
    "    for idx, words in enumerate(tokenized_test_texts):\n",
    "        word2tfidf = get_doc_tfidf(words, word2text_count, N)\n",
    "        # print(word2tfidf)\n",
    "        # PMI - determines significance of the word for the class\n",
    "        # TFIDF - determines significance of the word for the document\n",
    "        tot_pmi0 = [ words_pmis[0][w] * word2tfidf[w] for w in set(words) if w in words_pmis[0] ]\n",
    "        tot_pmi1 = [ words_pmis[1][w] * word2tfidf[w] for w in set(words) if w in words_pmis[1] ]\n",
    "        pmi0 = np.sum(tot_pmi0)\n",
    "        pmi1 = np.sum(tot_pmi1)\n",
    "        diff = pmi1 - pmi0\n",
    "        if diff > 0.006:\n",
    "            results[idx] = 1\n",
    "    return results\n",
    "\n",
    "def text_embeddings(text_tokenized, words_pmis, word2text_count, N):\n",
    "    embeddings = []\n",
    "    for words in text_tokenized:\n",
    "        word2tfidf = get_doc_tfidf(words, word2text_count, N)\n",
    "        embedding = torch.FloatTensor(np.zeros( nlp(text_tokenized[0][0]).vector.shape[0] + 2)).to(device)\n",
    "        pmi0 = 0;\n",
    "        pmi1 = 0;\n",
    "        for word in words:\n",
    "            embedding[:200] += nlp(word).vector\n",
    "            try:\n",
    "                pmi0 += words_pmis[0][word] * word2tfidf[word]\n",
    "                pmi1 += words_pmis[1][word] * word2tfidf[word]\n",
    "            except:\n",
    "                continue\n",
    "        embedding[-1] = pmi0\n",
    "        embedding[-2] = pmi1\n",
    "        embeddings.append(embedding / len(words))\n",
    "    return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "specialized-rogers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2841, 5)\n",
      "(11364, 5)\n"
     ]
    }
   ],
   "source": [
    "data = data_raw.sample(frac=1)\n",
    "idx = int(data.shape[0] * 0.2)\n",
    "test_data = data.iloc[:idx]\n",
    "train_data = data.iloc[idx:]\n",
    "print(test_data.shape)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subjective-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = tokenization(train_data)\n",
    "tokenized_test_texts = tokenization(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beginning-calculation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.71 s, sys: 8.1 ms, total: 1.72 s\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2text_count = get_word_stat( tokenized_texts )\n",
    "targets_train = train_data['Label'].values\n",
    "targets_test = test_data['Label'].values\n",
    "N = len(tokenized_texts)\n",
    "words_pmis = create_pmi_dict(tokenized_texts, targets_train, min_count=5)\n",
    "embeddings = text_embeddings(tokenized_texts[:4], words_pmis, word2text_count, N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
